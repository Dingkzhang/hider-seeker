{
    "index": 5,
    "month_int": "02",
    "month_name": "February",
    "year_int": "2020",
    "total_days": "29",
    "date_name_start": "Wednesday",
    "date_name_end": "Friday",
    "calendar_info": {
        "17": {
            "doesDataExist": true,
            "main_info": [
                {
                    "type": "text",
                    "paragraph": "Day 119: Flu A and Flu B woohoo"
                },
                {
                    "type": "text",
                    "paragraph": "Added weight update for input to hidden layer. (Not 100% sure I did it correctly.) Increased number of hidden layer weights."
                },
                {
                    "type": "text",
                    "paragraph": "Results are still diverging. Need to figure out why.... Need to look up commmon DQN mistakes."
                },
                {
                    "type": "text",
                    "paragraph": "Definitely someting wrong with hidden layer and input layer weight updates. Need to look at the math again and figure out what I did wrong."
                }
            ]
        },
        "16": {
            "doesDataExist": true,
            "main_info": [
                {
                    "type": "text",
                    "paragraph": "Day 118: Practicing in league uwu"
                },
                {
                    "type": "text",
                    "paragraph": "Need to increase the weight size, implement weight update for input to hidden layer, and look up common DQN mistakes."
                }
            ]
        },
        "15": {
            "doesDataExist": true,
            "main_info": [
                {
                    "type": "text",
                    "paragraph": "Day 117: Back to being sick >:( and just playing some league today"
                }
            ]
        },
        "14": {
            "doesDataExist": true,
            "main_info": [
                {
                    "type": "text",
                    "paragraph": "Day 116: Valentine's Day"
                },
                {
                    "type": "text",
                    "paragraph": "After adding function limit with relu activation function it seems to not diverge as much. However, after 50k iterations the error starts to increase again."
                },
                {
                    "type": "text",
                    "paragraph": "Changed to leaky relu which actually made the problem worse. Fuck. Still need to implment final layer of weight update..."
                }
            ]
        },
        "13": {
            "doesDataExist": true,
            "main_info": [
                {
                    "type": "text",
                    "paragraph": "Day 115: Limiting the experience_replay"
                },
                {
                    "type": "text",
                    "paragraph": "Added function to limit experience_replay to 1000 experiences max."
                }
            ]
        },
        "12": {
            "doesDataExist": true,
            "main_info": [
                {
                    "type": "text",
                    "paragraph": "Day 114: Fixing the divergence issue"
                },
                {
                    "type": "text",
                    "paragraph": "Wasn't really the learning rate that was the problem. I did make it smaller so it takes longer to diverge."
                },
                {
                    "type": "text",
                    "paragraph": "Don't have an experience limit right now. Might also be that the system is picking old data that it never gets rid of old experiences and that it interfering with learning."
                },
                {
                    "type": "text",
                    "paragraph": "Could potentially be the error function. The error is the sum to the power of 2 which will create huge errors. Not good for the system when it suddenly has to increment the weight by a huge amount. Another reason to get rid of old data that can cause variance."
                },
                {
                    "type": "text",
                    "paragraph": "Added weight update for hidden->hidden layer"
                }
            ]
        },
        "11": {
            "doesDataExist": true,
            "main_info": [
                {
                    "type": "text",
                    "paragraph": "Day 113: Running fun fun"
                },
                {
                    "type": "text",
                    "paragraph": "Think learning rate might be too big. So I set it to 0.001"
                },
                {
                    "type": "text",
                    "paragraph": "Fixed issue where I was only using policy weights and not using target weights for next state."
                },
                {
                    "type": "text",
                    "paragraph": "Ran 5.80 miles in 65:00"
                }
            ]
        },
        "10": {
            "doesDataExist": true,
            "main_info": [
                {
                    "type": "text",
                    "paragraph": "Day 112: OMG its diverging >:("
                },
                {
                    "type": "text",
                    "paragraph": "Writing back-propagation function. However, the function is diverging! Need to figure out the reason."
                }
            ]
        },
        "09": {
            "doesDataExist": true,
            "main_info": [
                {
                    "type": "text",
                    "paragraph": "Day 111: Didn't tilt very hard today. Good day :O"
                }
            ]
        },
        "08": {
            "doesDataExist": true,
            "main_info": [
                {
                    "type": "text",
                    "paragraph": "Day 110: Relaxed today and played league :)"
                }
            ]
        },
        "07": {
            "doesDataExist": true,
            "main_info": [
                {
                    "type": "text",
                    "paragraph": "Day 109: Working out the math"
                },
                {
                    "type": "text",
                    "paragraph": "Wrote out the back propagation math I need for each weight update."
                }
            ]
        },
        "06": {
            "doesDataExist": true,
            "main_info": [
                {
                    "type": "text",
                    "paragraph": "Day 108: Intimidated by NN >:()"
                },
                {
                    "type": "text",
                    "paragraph": "Learning how to map back propagation for Deep q Learning"
                }
            ]
        },
        "05": {
            "doesDataExist": true,
            "main_info": [
                {
                    "type": "text",
                    "paragraph": "Day 107: Working out back propagation equations"
                },
                {
                    "type": "text",
                    "paragraph": "Re-visited some more articles on back propagation."
                },
                {
                    "type": "text",
                    "paragraph": "Look at how other people handled activation functions and output layer"
                },
                {
                    "type": "text",
                    "paragraph": "Re-wrote code to use relu for 1st and 2nd hidden layer. Re-wrote code to use linear output instead of softmax for output layer."
                }
            ]
        },
        "04": {
            "doesDataExist": true,
            "main_info": [
                {
                    "type": "text",
                    "paragraph": "Day 106: Mistake tilt and in the ZONE (flow state)"
                },
                {
                    "type": "text",
                    "paragraph": "Read Mistake Tile and injustice tilt (Chapter 5) on The Mental Game of Pokwer by Jared Tendler"
                },
                {
                    "type": "text",
                    "paragraph": "Looked at several articles on how to do back-propagation for deep q learning."
                },
                {
                    "type": "text",
                    "paragraph": "Wrote loss and cost function (mean squared error)"
                },
                {
                    "type": "text",
                    "paragraph": "Ran 4.00 miles in 46:09"
                }
            ]
        },
        "03": {
            "doesDataExist": true,
            "main_info": [
                {
                    "type": "text",
                    "paragraph": "Day 105: Focus Please"
                },
                {
                    "type": "text",
                    "paragraph": "Continued reading chapter 5 on The Mental Game of Pokwer by Jared Tendler"
                },
                {
                    "type": "text",
                    "paragraph": "Read How to Become the Best Programmer in the World by Ohans Emmanuel"
                },
                {
                    "type": "text",
                    "paragraph": "Finished forward propagation and started working on cost function"
                }
            ]
        },
        "02": {
            "doesDataExist": true,
            "main_info": [
                {
                    "type": "text",
                    "paragraph": "Day 104: League of legends got to d3. :) Also, Must enter flow state for each and everyone of my goals >:("
                }
            ]
        },
        "01": {
            "doesDataExist": true,
            "main_info": [
                {
                    "type": "text",
                    "paragraph": "Day 103: Played a bunch of ranked games. Did not tilt. Good control good practice"
                },
                {
                    "type": "text",
                    "paragraph": "Read the beginning sections of Chapter 5 on The Mental Game of Poker by Jared Tendler"
                },
                {
                    "type": "text",
                    "paragraph": "Corrected forward propagation function. Need to add cost function and backward propagation next"
                }
            ]
        }
    }
}